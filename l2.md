
# Attention & Transformer (Lecture 2).

## 1. NLP (recap).

NLP не только про текст, а вообще про любые последовательности - видео, звук.

Общая проблема: receptive field ограничен, хоть и может быть довольно большим, но
до размеров книги никак не доходит.

### 1.1. RNN.

**Идея**: имеем матрицу "обновления" скрытого контекста из входного токена - $W_{xh}$,
а также матрицу забывания - $W_{hh}$. Тогда в каждый момент времени $t$ новое скрытое
состояние мы получаем путем забывания части старого скрытого состояния и
обновления за счет новой информации (токена $x_t$)

$$h_t = tanh(h_{t - 1}^{T}W_{hh} + x_{t}^{T}W_{xh})$$

Можно $h_t$ на каждом шаге выдавать в другой слой и получать $y_t$, то есть над
одной RNN-кой навесить другую RNN-ку обрабатывать выходы первой, тогда в $y_n$
будет более обогащенная информация, чем в $h_n$ и подавать в голову уже его.

Проблема:
* Не параллелится.
* Затухания/взрывы градиента (tanh, $\frac{\partial h_t}{\partial h_k}$ зависит от 
всех предыдущих градиентов).


Решения:
* Параллельность - не обсуждаем.
* Взрывы - gradient clipping.
* Затухания - skip-connections.

Однако, если использовать skip-connections, то возникает другая проблема - менять $h_t$
становится трудно, градиент по skip-connection-у будет $1$ и доминировать в глубокой RNN
(с потенциально затухающими весами) против изначального слагаемого. Более того, сама
концепция немного трется, так как матрицы $W_{xh}$ и $W_{hh}$ не имеют полного контроля
над $h_t$.

Но можно сделать умные skip-connections! Это делают в LSTM.

### 1.2. LSTM.

LSTM - настраиваемый skip-connection. $C_t$ - долгосрочная память, $h_t$ - 
краткосрочная память.

*Основное различие*: раньше мы выход предыдущего слоя $h_t$ подавали в
следующий слой как скрытое состояние. Теперь, давайте будем иметь "независимую"
компоненту скрытого состояния (долгосрочная память), которая никуда не выводится,
а лишь используется для "внутреннего пользования".

На каждом шаге мы:

1. Forget Gate - забываем часть содержимого в долгосрочной памяти ($C_{t - 1}$).

$$f_t = \sigma(W_f[h_{t - 1}, x_t] + b_f)$$

2. Input Gate - добавляем полученную информацию в $\hat{C}_t$.

$$i_t = \sigma(W_i[h_{t - 1}, x_t] + b_i)$$

3. Output Gate - выводим часть памяти $C_t$ на выход $h_t$.

$$o_t = \sigma(W_o[h_{t - 1}, x_t] + b_o)$$

Далее мы используем каждый Gate для операции над $C_{t - 1}$:

* Компонента, которую должны добавить (поэтому $tanh$):

$$\hat{C}_t = \tanh(W_C[h_{t - 1}, x_t] + b_C)$$

* Обновленная память (часть забыли и часть добавили):

$$C_t = f_t * C_{t - 1} + i_t * \hat{C}_t$$

* Выход на данном шаге:

$$h_t = o_t * C_t$$

По сути реализовали обучаемые skip-connections в RNN.

Проблема:
* Также не параллелится.

## 2. Виды задач.

### 2.1. One2One.

Не рекуррентная информация $\mapsto$ не рекуррентная информация,
не интересно (не рекуррентная сеть).

### 2.2. Seq2One.

Рекуррентная информация $\mapsto$ не рекуррентная информация.

Классификация по тексту, например. Выдать выход рекуррентной сети,
как вариант.

### 2.3. One2Seq.

Не рекуррентная информация $\mapsto$ рекуррентная информация.

* Image Captioning - по картинке генерить описание.
* Генерировать музыку по параметрам.

### 2.4. Seq2Seq Same.

По последовательности получить последовательность того же размера.

Можно также через рекурренту, только из выдать последовательность выходов
сети на каждом слое.

* Name-entity recognition - разметить части предложения.

### 2.5. Seq2Seq Different.

По последовательности получить последовательность другого размера.

* Перевод текста.
* Генерация продолжения текста.

## 3. Attention.

### 3.1. Интуиция.

Все RNN-подобные архитектуры не могут держать большой контекст в голове.

### 3.2. На примере Seq2Seq.

Пусть мы генерируем продолжение текста через RNN. Тогда мы загоняем исходную
последовательность в сетку (encoder), а затем генерируем токен и подаем его
в качестве входа на следующий слой (decoder). Эта связь называется
"авторегрессионной". На обучении можно подавать не предсказанный, а верный токен.
Такая стратегия называется Teacher Forcing.

**Концепция**: пусть теперь мы хотим при формировании ответа смореть не на какое-то промежуточное
представление, а целиком на все предыдущие $h_i$. Тогда можем сделать следующее.

Через $z$ обозначим настоящее скрытое состояние $h_t$, которое в предыдущем примере
подавалось в классификационную голову. Определим релевантность $s_i$ для
$i = 0, ..., t - 1$:

$$
\begin{equation}
s_i = score_i(h_i, z) = 
\begin{cases}
h_i^Tz & \in \mathbb{R} \\
h_i^TW_{att}z & \in \mathbb{R} \\
W_{att}[h_i;z] & \in \mathbb{R}^k \\
\forall\text{обучаемое }f(h_i, z) & \in \mathbb{R}, \mathbb{R}^k\\
\end{cases}
\end{equation}
$$

Полученные $s_i$ интрерпретируем как логиты, считаем вероятности:

$$a = softmax(s)$$

Контекст получаем как взвешенную сумму:

$$c = \sum_{i}a_i h_i$$

Новые признаки (выход вместо просто $h_t$ для получения $y_t$):

$$\tilde{z} = [c;z]$$

## 4. Transformer.

Мы создали Attention, но это все еще сделано поверх рекуррентной архитектуры.

Давайте сразу для каждого токена считать важность всех остальных токенов. То
есть не делать какую-то рекурренту.

### 4.1. Attention Is All You Need.

Encoder-Decoder архитектура.

Незнакомые элементы:

* Multi-Head Attention.
* Masked Multi-Head Attention.
* Positional Encoding.

#### 4.1.2. Multi-Head Attention.

По входу ($X \in \mathbb{R}^{seq \times emb}$) хотим оценить важность
каждого токена для всех остальных токенов.

Применим независимые линейные слои для получения трех разных матриц из $X$:

$$W^Q, W^K, W^V \in \mathbb{R}^{emb \times d}$$
* $Q = XW^Q \in \mathbb{R}^{seq \times d_Q}$ - query.
* $K = XW^K \in \mathbb{R}^{seq \times d_K}$ - key.
* $V = XW^V \in \mathbb{R}^{seq \times d_V}$ - value.
* $d, d_Q, d_K, d_V$ - внутренние размерности, должны быть одинаковы для $Q$ и $K$,
для $V$ размерность может быть другой, но в статье решили сделать общей для всех.

Считаем релевантность для всех пар токенов
$$S = QK^T \in \mathbb{R}^{seq \times seq}$$

то есть $S_{ij}$ - релевантность $i$-ого токена (берется $i$-ая строка из $Q$) к
$j$-ому токену (берется $j$-строка из $K$).

Это дополнительно нормализуется на $\sqrt{d}$, так как при повышении $d$
каждая ячейка матрицы получается путем суммирования большего числа компонент,
и тогда растет magnitude ячеек. Далее берется $softmax$
для получения вероятностей/весов (по строчкам):

$$A = Softmax\left(\frac{QK^T}{\sqrt{d}}\right)$$

Финально, считаем сами признаки:

$$Attention(Q, K, V) = Softmax\left(\frac{QK^T}{\sqrt{d}}\right)V \in \mathbb{R}^{seq \times d_V}$$

от есть в ячейке $ij$ лежит скалярное произведение $i$-ой строки весов $A$ и $j$-ого
столбца $V$. Помним, что $i$-ая строка $V$ означает вектор значений для $i$-ого токена.
По сути, мы из $V$ взяли $j$-ую компоненту всех токенов, и с весами $i$-ой строки весов $A$
их сложили. А тогда если теперь рассмотрим не ячейку $ij$ выхода, а целиком $i$-ую
строку, то в ней лежит сумма строк $V$ с весами $i$-ой строки весов $A$.

Выход $Attention$-а назовем "головой":

$$head_i = Attention(XW_i^Q, XW_i^K, XW_i^V)$$

Сделаем $h$ разных $Attention$-ов, сконкатенируем результат и добавим переходную
матрицу $W^O \in \mathbb{R}^{h\cdot d_V \times d_O}$. Тогда получим многоголовый
$Attention$:

$$MultiHead = Concat(head_1, ..., head_h)W^O \in \mathbb{R}^{seq \times d_O}$$


#### 4.1.3. Positional Encoding.

Сам механизм $Attention$, описанный выше, никак не учитывает позицию токенов. Более того,
мы хотим кодировать не только индекс токена, но еще и компоненту эмбеддинга этого токена.

Легче всего представить это следующим образом: рассмотрим входную матрицу $Attention$-а 
$\in \mathbb{R}^{seq \times emb}$. Вдоль оси seq позиция очевидна важна - так как порядок токенов
важен. Однако, важна еще и позиция вдоль оси emb, без нее можно безболезненно переставлять
столбцы входной матрицы. Почему это нужно? Мы задаем это на уровне архитектуры нейросети. Иными
словами, это необходимость не с точки зрения входных данных (которые обладают временной
характеристикой), а с точки зрения того, как мы хотим чтобы работала наша модель.
*Подробнее на семинаре.*

### 4.2. Encoder.

Эта часть архитектуры отвечает за то, чтобы прочитать входные данные и представить их в каком-то
внутреннем информативном пространстве.

Encoder получает эмбеддинги токенов - матрицу $X \in \mathbb{R}^{seq \times emb}$.

Прибавляет к ним positional encoding - получают матрицу
такой же размерности $\tilde{X} \in \mathbb{R}^{seq \times emb}$.

Затем повторяет $N$ раз:
* Multi-Head Attention, $\tilde{X}_1 = f(\tilde{X}) \in \mathbb{R}^{seq \times d_O}$.
* Skip + Normalization, $\tilde{X}_2 = LayerNorm(\tilde{X} + f(\tilde{X})) =
LayerNorm(\tilde{X} + \tilde{X}_1) \in \mathbb{R}^{seq \times d_O}$.
* Feed Forward,         $\tilde{X}_3 = g(\tilde{X}_2) \in \mathbb{R}^{seq \times d_O}$.
* Skip + Normalization, $\tilde{X}_4 = LayerNorm(\tilde{X}_2 + g(\tilde{X}_2)) =
LayerNorm(\tilde{X}_2 + \tilde{X}_3) \in \mathbb{R}^{seq \times d_O}$.

### 4.3. Decoder.

Декодер получает на вход эмбеддинги токенов предсказанных ранее (все еще авторегрессия).

Прибавляет к ним позиционную информацию.

Затем повторяет $N$ раз (только отличия от encoder-а):
* Masked Multi-Head Attention: умножаем на маску, чтобы нельзя было
смотреть вперед.
* Второй Multi-Head (Cross) Attention получает $K$ и $V$ из энкодера, а $Q$
берет из прошлого слоя декодера. Это означает, $K = X^{Encoder}W^K,
V = X^{Encoder}W^V$, а $Q = X^{Decoder}W^Q$, где $X^{Encoder} \in
\mathbb{R}^{seq_e \times emb}, X^{Decoder} \in \mathbb{R}^{seq_d \times emb}$.
Тогда $QK^T \in \mathbb{R}^{seq_d \times seq_e}$. Мы смотрим на важность $i$-ого элемента
decoder-а к $j$-ому элементу encoder-а. Следовательно:

$$Softmax\left(\frac{QK^T}{\sqrt{d}}\right)V \in \mathbb{R}^{seq_d \times d_V}$$

### 4.4. Training/Inference.

Вход decoder-а:

$$
\begin{array}{|c|c|}
\text{Вход} & bos, y_1, ..., y_N \\
\text{Таргет}   & y_1, ..., y_N, eos \\
\end{array}
$$

На инференсе в качестве таргета подается только bos. Авторегрессия при обучении формально отсутствует,
так как в отличии от RNN где каждый токен обрабатывается последовательно, мы одновременно
работаем со всей входной последовательностью. А вот при генерации уже без авторегрессии
не обойтись.

## 5. Современный NLP.

### 5.1. Какие бывают модели?

* Encoder-only: BERT, RoBERTa.
* Decoder-only: T5, BART.
* Encoder-Decoder: генеративные модели (GPT, Llama).

### 5.2. BERT.

BERT = Bidirectional Encoder Representations from Transformers.

Хотим получать качественные эмбеддинги текстов. Для этого encoder часть будет давать нам
хорошее внутренне представление текста, далее уже будем решать целевую задачу.

**Обучение**: Masked Language Modelling (MLM) или Next Sentence Prediction (NSP). Выбрано такое,
так как изначально целевой задачи у нас нет - нужно просто эмбеддить текст.

#### 5.2.1. MLM.

Идея: часть токенов выкидываем, пытаемся их предсказать по остальным.

* Маскируем токены в текстах, заменяя их на $[MASK]$ ($90$%) либо случайное слово ($10$%),
подаем в encoder.
* После encoder-а мы имеем ранее упомянутю матрицу $\mathbb{R}^{seq \times d_O}$, то есть каждому
изначальному токену сопоставлен какой-то вектор длины $d_O$.
* Над этим навесим классификационную голову и обычной Cross Entropy будем обучать модель,
ожидая, что вместо $[MASK]$ она будет выдавать нужный токен.
