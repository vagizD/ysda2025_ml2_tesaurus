

# Reinforcement Learning (Lecture 3).

Основная задача RL: процесс принятия решений. Есть "агент" и "среда". Агент
принимает решение, посылает его в среду, и она как-то реагирует на это.

Пусть мы хотим решать задачу self-driving cars: по картинке с камеры автомобиля
предсказывать вектор направления. Можно ли это делать в парадигме supervised-learning?
Почти всегда в supervised-learning мы говорили, что наши данные iid, то есть наблюдения из
выборки X являеются независимо сэмплированными из некоторой генеральной совокупности.

#### Проблема Distributional Shift

Проблема того, что обученный агент может зайти туда, где эксперт никогда не был. То есть
наблюдения меняются с изменением стратегии (поведения агента). В supervised train/test из
одного распределения, однако, в RL train взят из одного распределения, а test - распределения
под действиями агента.



##### Dagger

Dataset Aggregation. Есть агент и эксперт (тот же self-driving cars). Собираем датасет действий $a_i$
в ответ на ситуации $s_i$. Пока не удовлетворены:

1. Обучаем модель $\pi: a_i \approx \pi(s_i)$.
2. Тестируем агента в среде (пускаем поездить по городу), записываем его
траектории $\tilde{s}_i$.
3. Просим эксперта дать верные ответы на ситуации $\tilde{a}_i$ (вектора направлений в городе).
4. Добавляем $\{(\tilde{s}_i, \tilde{a}_i)\}$ в выборку.

Обычно делают более умно - водитель сразу в машине сидит, беспилотный автомобиль сам едет,
и как только он начинает делать что-то не то, водитель перехватывает управление и с этого момента
идет запись, потом на этом дообучаются.

Минусы:
* Эксперт нужен в режиме онлайн (долго и дорого).
* Агент не будет лучше эксперта.
* Эксперта может не быть (пример - робота учат ходить).

### Виды сред

Развилка в RL обычно строится на том, нужна в задаче история или нет.

#### Марковость

Будущее не зависит от прошлого, если известно настоящее. Превратить среду в Марковскую можно
в том или ином виде (если процесс конечный).

## Постановка задачи

Следующее обозначают как Markov Decision Process (MDP) как $(S, A, p_0, p, r)$:

* $S$ - пространство состояний/наблюдений.
* $A$ - пространство действий.
* $p_0(s)$ - начальное распределение (откуда среда порождает новые состояния).
* $p(s'|s, a)$ - Марковская вероятность переходов.
* $r(s', s, a)$ - награда.
* $\pi(a|s)$ - политика (стратегия).
* $\tau = (s_0, a_0, s_1, a_1, ..., s_n, a_n)$.

Если награда зависит от прошлого, то среда "не Марковская по награде".

$$p(\tau | \pi) = p_0(s_0)\prod_{t=0}^{\infty}\pi(a_t|s_t)p(s_{t + 1}|s_t, a_t)$$

В среде и политике есть какая-то стохастика. Следовательно, надо максимизировать
какое-то матожидание. Для Марковских сред:

$$\mathbb{E}_{\tau \sim p(\tau | \pi)}\sum_{t=0}^{\infty}r(s_{t + 1}, s_t, a_t) \mapsto \max_{\pi}$$


#### Как выбрать награду?

Давать награду только за победу в шахматах - целевая, но сложная. Давать промежуточную
награду за взятие фигур - не целевая, но проще. Поэтому обычно итеративно
предобучают на простых нецелевых наградах, а по ходу дела менют их на более сложные.

#### Дисконтирование награды

Нужно обойти проблему того, что агент может откладывать решение задачи и получать такую же награду:

* Ввести exponential decay на награду, зависящую от времени.
* Сделать энергетические затраты за ход.
